# This eksctl configuration file represents the cluster and node groups for use
# by the cluster.
# ref: https://eksctl.io/usage/schema/
#
# Cluster operations:
# ref: https://eksctl.io/usage/cluster-upgrade/
#
#   create:   eksctl create cluster --config-file=eksctl-cluster-config.yaml --set-kubeconfig-context
#   upgrade:  eksctl upgrade cluster --config-file=eksctl-cluster-config.yaml
#   delete:   eksctl delete cluster --config-file=eksctl-cluster-config.yaml
#
# Node group operations:
# ref: https://eksctl.io/usage/managing-nodegroups/
#
#   eksctl create nodegroup --config-file=eksctl-cluster-config.yaml --include user-b,worker-b
#   eksctl delete nodegroup --cluster l2l --name user-a
#
# Attribution: this was based on @yuvipanda's work in 2i2c! <3
# ref: https://github.com/2i2c-org/pangeo-hubs/blob/8e552bc198d8339efe8c003cb847849255e8f8ed/aws/eksctl-config.yaml
#

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: l2l
  # region:
  #   The region was chosen to to us-west-2 (Oregon) over us-east-2 (Ohio) which
  #   was another reasonable option for the sake of data locality, see what to
  #   consider in this great guide:
  #   https://www.concurrencylabs.com/blog/choose-your-aws-region-wisely/
  region: us-west-2
  version: "1.18"
# availabilityZones: 
#   For the EKS control plane, arbitrary chosen but made explicit to ensure we
#   can locate the node pool on an AZ where the EKS control plane exist as
#   required.
availabilityZones: [us-west-2d, us-west-2b, us-west-2a]

# This section will create additional k8s ServiceAccount's that are coupled with
# AWS Role's. By declaring pods to use them, you can grant these pods the
# associated permissions. For this deployment, we create a k8s ServiceAccount
# with Full S3 credentials which we then also declare user pods and dask worker
# pods will make use of.
iam:
  withOIDC: true        # https://eksctl.io/usage/security/#withoidc
  # serviceAccounts like nodeGroups etc can be managed directly with eksctl, for
  # more information, see: https://eksctl.io/usage/iamserviceaccounts/
  #
  #   eksctl create iamserviceaccount --config-file=eksctl-cluster-config.yaml
  #
  serviceAccounts:
    - metadata:
        name: s3-full-access
        namespace: default
        labels: {aws-usage: "application"}
      attachPolicyARNs:
        - "arn:aws:iam::aws:policy/AmazonS3FullAccess"

# Choose the type of node group?
# - nodeGroups cannot be updated but must be recreated on changes:
#   https://eksctl.io/usage/managing-nodegroups/#nodegroup-immutability
# - managedNodeGroups cannot scale to zero:
#   https://github.com/aws/containers-roadmap/issues/724
#
# Choosing machine type?
# - Maximum pods: https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt
# - Node specs:   https://aws.amazon.com/ec2/instance-types/
#
# Management advice:
# - Always use a suffix for node group names that you can replace with something
#   to create a new node group and delete the old. You will run into issues if
#   you name it "core" and "core-a" instead of "core-a" and "core-b", such as
#   when deleting "core" you end up draining both node groups.
nodeGroups:
  - name: core-a
    availabilityZones: [us-west-2d]   # aws ec2 describe-availability-zones --region <region-name>
    instanceType: m5.large   # 28 pods, 2 cpu, 8 GB
    minSize: 0
    maxSize: 2
    desiredCapacity: 1
    volumeSize: 80
    labels:
      hub.jupyter.org/node-purpose: core
    # tags: we need to help the cluster autoscaler a bit to know what labels the
    #       nodes created from a node group will get, besides knowing if a k8s
    #       cluster can use a node group.
    # ref: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#auto-discovery-setup
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: core
      # k8s.io/cluster-autoscaler/<cluster-name>  - automaticly added by eksctl!
      # k8s.io/cluster-autoscaler/enabled         - automaticly added by eksctl!
    iam:
      withAddonPolicies:
        autoScaler: true

  - name: user-a
    availabilityZones: [us-west-2d]
    instanceType: m5.2xlarge   # 57 pods, 8 cpu, 32 GB
    minSize: 0
    maxSize: 20
    desiredCapacity: 0
    volumeSize: 80
    labels:
      hub.jupyter.org/node-purpose: user
    tags:
      k8s.io/cluster-autoscaler/node-template/label/hub.jupyter.org/node-purpose: user
    iam:
      withAddonPolicies:
        autoScaler: true

  # Important about spot nodes!
  #
  # "Due to the Cluster Autoscaler’s limitations (more on that in the next
  # section) on which Instance type to expand, it’s important to choose
  # instances of the same size (vCPU and memory) for each InstanceGroup."
  #
  # ref: https://medium.com/riskified-technology/run-kubernetes-on-aws-ec2-spot-instances-with-zero-downtime-f7327a95dea
  #
  - name: worker-xlarge
    availabilityZones: [us-west-2d, us-west-2b, us-west-2a]
    minSize: 0
    maxSize: 20
    desiredCapacity: 0
    volumeSize: 80
    labels:
      worker: "true"
    taints:
      worker: "true:NoSchedule"
    tags:
      k8s.io/cluster-autoscaler/node-template/label/worker: "true"
      k8s.io/cluster-autoscaler/node-template/taint/worker: "true:NoSchedule"
    iam:
      withAddonPolicies:
        autoScaler: true
    # Spot instance configuration
    instancesDistribution:  # ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-autoscaling-autoscalinggroup-instancesdistribution.html
      instanceTypes:
        - m5.xlarge    # 57 pods, 4 cpu, 16 GB
        - m5a.xlarge    # 57 pods, 4 cpu, 16 GB
        - m5n.xlarge    # 57 pods, 4 cpu, 16 GB
      onDemandBaseCapacity: 0
      onDemandPercentageAboveBaseCapacity: 0
      spotAllocationStrategy: "capacity-optimized"  # ref: https://aws.amazon.com/blogs/compute/introducing-the-capacity-optimized-allocation-strategy-for-amazon-ec2-spot-instances/

  - name: worker-2xlarge
    availabilityZones: [us-west-2d, us-west-2b, us-west-2a]
    minSize: 0
    maxSize: 20
    desiredCapacity: 0
    volumeSize: 80
    labels:
      worker: "true"
    taints:
      worker: "true:NoSchedule"
    tags:
      k8s.io/cluster-autoscaler/node-template/label/worker: "true"
      k8s.io/cluster-autoscaler/node-template/taint/worker: "true:NoSchedule"
    iam:
      withAddonPolicies:
        autoScaler: true
    # Spot instance configuration
    instancesDistribution:  # ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-autoscaling-autoscalinggroup-instancesdistribution.html
      instanceTypes:
        - m5.2xlarge   # 57 pods, 8 cpu, 32 GB
        - m5a.2xlarge   # 57 pods, 8 cpu, 32 GB
        - m5n.2xlarge   # 57 pods, 8 cpu, 32 GB
      onDemandBaseCapacity: 0
      onDemandPercentageAboveBaseCapacity: 0
      spotAllocationStrategy: "capacity-optimized"  # ref: https://aws.amazon.com/blogs/compute/introducing-the-capacity-optimized-allocation-strategy-for-amazon-ec2-spot-instances/

  - name: worker-4xlarge
    availabilityZones: [us-west-2d, us-west-2b, us-west-2a]
    minSize: 0
    maxSize: 20
    desiredCapacity: 0
    volumeSize: 80
    labels:
      worker: "true"
    taints:
      worker: "true:NoSchedule"
    tags:
      k8s.io/cluster-autoscaler/node-template/label/worker: "true"
      k8s.io/cluster-autoscaler/node-template/taint/worker: "true:NoSchedule"
    iam:
      withAddonPolicies:
        autoScaler: true
    # Spot instance configuration
    instancesDistribution:  # ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-autoscaling-autoscalinggroup-instancesdistribution.html
      instanceTypes:
        - m5.4xlarge   # 233 pods, 16 cpu, 64 GB
        - m5a.4xlarge   # 233 pods, 16 cpu, 64 GB
        - m5n.4xlarge   # 233 pods, 16 cpu, 64 GB
      onDemandBaseCapacity: 0
      onDemandPercentageAboveBaseCapacity: 0
      spotAllocationStrategy: "capacity-optimized"  # ref: https://aws.amazon.com/blogs/compute/introducing-the-capacity-optimized-allocation-strategy-for-amazon-ec2-spot-instances/