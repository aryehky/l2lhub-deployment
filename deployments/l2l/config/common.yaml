tags:
  # Controls whether Prometheus and Grafana should be be installed as part of
  # this meta Helm chart. The chart is configured to use them if they are
  # available. Its also worth noting that access to Grafana relies on
  # JupyterHub's configurable proxy in this setup, so JupyterHub needs to run to
  # be able to access Grafana.
  metrics: true
  dask-gateway: true



# Reference on the configuration options:
# https://github.com/dask/helm-chart/blob/master/daskhub/values.yaml
daskhub:
  # Reference on the configuration options:
  # https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/master/jupyterhub/values.yaml
  jupyterhub:
    auth:
      whitelist:
        users:
          - arokem
          - consideratio
      admin:
        access: false
        users:
          - arokem
          - consideratio

      type: github
      github:
        # clientId is set in secrets folder
        # clientSecret is set in secrets folder
        callbackUrl: https://l2l.sundellopensource.com/hub/oauth_callback

    prePuller:
      hook:
        enabled: true
      continuous:
        enabled: true

    scheduling:
      userScheduler:
        enabled: true
        replicas: 2
      podPriority:
        enabled: true
      userPlaceholder:
        enabled: true
        # These replicas will use the default configured CPU and memory requests,
        # which is overridden by c.KubeSpawner.profile_list choices by the user.
        replicas: 0
      corePods:
        nodeAffinity:
          matchNodePurpose: require
      userPods:
        nodeAffinity:
          matchNodePurpose: require

    singleuser:
      defaultUrl: /lab
      startTimeout: 900
      cpu:  # defaults for when profile_list is bypassed, for example by user-placeholder pods
        guarantee: 0.5
        limit: 16
      memory:
        guarantee: 6G
        limit: 12G
      lifecycleHooks:
        postStart:
          exec:
            command:
              - "bash"
              - "/etc/singleuser/k8s-lifecycle-hook-post-start.sh"
      storage:
        storage:
          capacity: 10Gi
        extraVolumes:
          ## Comment out the nh-nfs volume if nfs.enabled: false
          # - name: nh-nfs
          #   persistentVolumeClaim:
          #     claimName: nfs-pvc
          - name: user-etc-singleuser
            configMap:
              name: user-etc-singleuser
          - name: user-etc-profile-d
            configMap:
              name: user-etc-profile-d
          - name: user-usr-local-etc-jupyter
            configMap:
              name: user-usr-local-etc-jupyter
        extraVolumeMounts:
          ## Comment out the nh-nfs volume if nfs.enabled: false
          # - name: nh-nfs
          #   mountPath: /nh/curriculum
          #   subPath: curriculum
          #   readOnly: true
          - mountPath: /etc/singleuser
            name: user-etc-singleuser
          - mountPath: /etc/profile.d/home-folder-replacements.sh
            name: user-etc-profile-d
            subPath: home-folder-replacements.sh
          - mountPath: /usr/local/etc/jupyter
            name: user-usr-local-etc-jupyter

    hub:
      # Disabled because of dask-gateway Helm chart 0.9.0 requires a fix before it
      # works: https://github.com/dask/dask-gateway/pull/352
      networkPolicy:
        enabled: false

      # NOTE: readinessProbe is disabled because of bad default values make a hub
      # under load become unavailable.
      #
      # ref: https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues/1732
      readinessProbe:
        enabled: false

      # ref: https://jupyterhub.readthedocs.io/en/stable/reference/services.html#properties-of-a-service
      services:
        grafana:
          # This will make the CHP proxy let /services/grafana route to the
          # grafana service in the k8s namespace, which lets us make use of
          # JupyterHub's HTTPS setup without needing something like nginx-ingress
          # + cert-manager and additional ingress k8s resources.
          url: http://grafana

      resources:
        requests:
          cpu: 50m
          memory: 1Gi
        limits:
          cpu: 1000m
          memory: 1Gi

      pdb:
        enabled: false

      extraVolumes:
        - name: hub-etc-jupyterhub-templates
          configMap:
            name: hub-etc-jupyterhub-templates
        - name: hub-usr-local-share-jupyterhub-static-external
          configMap:
            name: hub-usr-local-share-jupyterhub-static-external
      extraVolumeMounts:
        - mountPath: /etc/jupyterhub/templates
          name: hub-etc-jupyterhub-templates
        - mountPath: /usr/local/share/jupyterhub/static/external
          name: hub-usr-local-share-jupyterhub-static-external

      extraConfig:
        announcements: |
          c.JupyterHub.template_vars.update({
              'announcement': 'Under development by Erik Sundell, please feel free to contact me on the l2l slack!',
          })
        performance: |
          c.JupyterHub.concurrent_spawn_limit = 100
        auth: |
          # Don't wait for users to press the orange button to login.
          c.Authenticator.auto_login = True
        templates: |
          # Help JupyterHub find the templates we may mount
          c.JupyterHub.template_paths.insert(0, "/etc/jupyterhub/templates")
        metrics: |
          # With this setting set to False, the /hub/metrics endpoint will be
          # publically accessible just like at hub.mybinder.org/hub/metrics is.
          c.JupyterHub.authenticate_prometheus = False
        workingDir: |
          # Override the working directory of /src/repo which repo2docker have set
          # to /home/jovyan instead, where we mount of files.
          c.KubeSpawner.extra_container_config = {
              "workingDir": "/home/jovyan",
          }
        options_form: |
          # Configure what spawn options users should see
          # ---------------------------------------------
          #
          # NOTE: setting c.KubeSpawner.profile_list directly is easier, but then
          #       we don't have the option to adjust it based on the individual
          #       user at a later point in time if we want.
          #
          # NOTE: c.KubeSpawner.options_form, defined in the Spawner base class,
          #       can be set to a fixed value, but it can also be a callable
          #       function that returns a value. If this returned value is falsy,
          #       no form will be rendered. In this case, we setup a callable
          #       function that relies on KubeSpawner's internal logic to create
          #       an options_form from the profile_list configuration.
          #
          #       ref: https://github.com/jupyterhub/jupyterhub/pull/2415
          #       ref: https://github.com/jupyterhub/jupyterhub/issues/2390
          #
          async def dynamic_options_form(self):
              self.profile_list = [
                  {
                      'default': True,
                      'display_name': 'Small',
                      'description': '6GB RAM',
                      'kubespawner_override': { 'mem_guarantee':' 6G' },
                  },
                  {
                      'display_name': 'Medium',
                      'description': '12GB RAM',
                      'kubespawner_override': { 'mem_guarantee': '12G' },
                  },
                  {
                      'display_name': 'Large',
                      'description': '24GB RAM',
                      'kubespawner_override': { 'mem_guarantee': '24G' },
                  },
              ]

              # NOTE: We let KubeSpawner inspect profile_list and decide what to
              #       return, it will return a falsy blank string if there is no
              #       profile_list, which makes no options form be presented.
              #
              # ref: https://github.com/jupyterhub/kubespawner/blob/37a80abb0a6c826e5c118a068fa1cf2725738038/kubespawner/spawner.py#L1885-L1935
              #
              return self._options_form_default()

          c.KubeSpawner.options_form = dynamic_options_form
        pre_spawn_hook: |
          # Configure details based on the user and profile chosen
          # ------------------------------------------------------
          #
          async def pre_spawn_hook(spawner):
              username = spawner.user.name
              user_options = spawner.user_options # {'profile': 'display_name of chosen profile'}

          c.KubeSpawner.pre_spawn_hook = pre_spawn_hook

    proxy:
      pdb:
        enabled: false
      https:
        enabled: true
        hosts: [l2l.sundellopensource.com]
      service:
        type: LoadBalancer
      chp:
        resources:
          requests:
            memory: 320Mi
            cpu: 50m
          limits:
            memory: 320Mi
            cpu: 500m
      traefik:
        resources:
          requests:
            memory: 512Mi
            cpu: 50m
          limits:
            memory: 512Mi
            cpu: 1000m

    cull:
      enabled: true
      # NOTE: This should probably be set to a value lower than or equal to 3600
      #       seconds given that its easy to startup later, notebooks are
      #       automatically saved, and it won't shutdown if something is running.
      timeout: 1800
      # NOTE: To have this at zero is probably a very bad idea as it make us fail
      #       to scale down nodes. Typically there is always one straggler on a
      #       node stuck in some code execution that doesn't end if it has housed
      #       a hundred of users.
      maxAge: 0 # Allow pods to run forever



  # Reference on the configuration options:
  # https://github.com/dask/dask-gateway/blob/master/resources/helm/dask-gateway/values.yaml
  dask-gateway:
    gateway:
      prefix: "/services/dask-gateway"  # Connect to Dask-Gateway through a JupyterHub service.
      auth:
        type: jupyterhub  # Use JupyterHub to authenticate with Dask-Gateway
      backend:
        scheduler:
          extraPodConfig:
            nodeSelector:
              hub.jupyter.org/node-purpose: user
            tolerations:
              - effect: NoSchedule
                key: hub.jupyter.org_dedicated
                operator: Equal
                value: user

        worker:
          extraPodConfig:
            nodeSelector:
              worker: "true"
            tolerations:
              - effect: NoSchedule
                key: worker
                operator: Equal
                value: "true"

    traefik:
      service:
        type: ClusterIP  # Access Dask-Gateway through JupyterHub.



# Reference on the Grafana Helm chart's configuration options:
# https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
grafana:
  # Reference on Grafana's configuration options:
  # https://grafana.com/docs/grafana/latest/administration/configuration/
  grafana.ini:
    # log:
    #   level: debug
    security:
      cookie_secure: true
      # secret_key is set in secrets folder
    server:
      domain: l2l.sundellopensource.com
      # NOTE: Don't use %(protocol)s in root_url, but hardcode https. If not, it
      #       will when redirecting the user to external authentication set with
      #       a redirect back query parameter to use http instead of https,
      #       which will be wrong. This is because the TLS termination is done
      #       without Grafanas knowledge by the ingress controller. If we would
      #       specify protocol to be https, then it would want to do the TLS
      #       termination itself so that also would fail.
      root_url: 'https://%(domain)s/services/grafana'
      serve_from_sub_path: true
      enforce_domain: true
      enable_gzip: true
      router_logging: true

  fullnameOverride: grafana
  adminUser: admin
  imageRenderer:
    enabled: true

  # NOTE: We need Recreate when using a persistence PVC. If we use an external
  # database, we can do a RollingUpdate instead.
  deploymentStrategy:
    type: Recreate

  persistence:
    type: pvc
    enabled: true

  service:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/path: "/services/grafana/metrics"

  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 50m
      memory: 100Mi

  initChownData:
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 25m
        memory: 64Mi



# Reference on the configuration options:
# https://github.com/helm/charts/blob/master/stable/prometheus/values.yaml
prometheus:
  fullnameOverride: prometheus

  # the actual prometheus server that polls various sources for metrics etc.
  server:
    fullnameOverride: prometheus-server
    enabled: true

    # data retention period
    retention: 3y

    # NOTE: We prefer StatefulSet to be used when using a persistence PVC. If we
    #       use an external database, we can use a Deployment with rolling
    #       updates instead. Until then, we should shut down one pod and then
    #       start up another, which a StatefulSet will do by default and a
    #       Deployment will Recreate as an upgradeStrategy will also do.
    statefulSet:
      enabled: true
    persistentVolume:
      enabled: true
      size: 200Gi
    resources:
      limits:
        cpu: 2
        memory: 12Gi
      requests:
        cpu: 50m
        # IMPORTANT: This value was lowered to 100Mi from 12Gi after the course
        # ended to allow prometheus to run in a cheaper node.
        memory: 100Mi

  # alertmanager is meant to be able to alert using email etc. Grafana can also
  # do this by itself to some degree at least as I understand it.
  alertmanager:
    fullnameOverride: prometheus-alertmanager
    enabled: false

  # kube-state-metrics exports information coming from the kubernetes api-server
  # about the state of kubernetes resources. It can list the state of pods etc.
  #
  # ref: https://github.com/helm/charts/blob/master/stable/prometheus/requirements.yaml
  # ref: https://github.com/helm/charts/tree/master/stable/kube-state-metrics
  kube-state-metrics:
    fullnameOverride: prometheus-kube-state-metrics
    resources:
      limits:
        cpu: 100m
        memory: 64Mi
      requests:
        cpu: 10m
        memory: 32Mi
  kubeStateMetrics:
    enabled: true

  nodeExporter:
    fullnameOverride: prometheus-node-exporter
    enabled: true
    # NOTE: We want to be able to scrape metrics on all nodes, even GPU nodes
    #       etc.
    tolerations:
      - operator: "Exists"
    resources:
      limits:
        cpu: 200m
        memory: 50Mi
      requests:
        cpu: 50m
        memory: 30Mi

  # pushgateway is meant to buffer metrics pushed to it from short lived sources
  # and expose them later for prometheus in their place.
  pushgateway:
    fullnameOverride: prometheus-pushgateway
    enabled: false
